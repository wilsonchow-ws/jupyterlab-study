{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42abe8f4-ae05-4303-8949-b5a8f7e970b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a lab test from DataAnalisysWith PySpark\n",
    "##<font size=\"12\">Chapter 2</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29a0089d-b63d-469a-8139-bdf1736dd79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# end-of-chapter.py############################################################\n",
    "#\n",
    "# Use this to get a free pass from Chapter 2 to Chapter 3.\n",
    "#\n",
    "# Remember, with great power comes great responsibility. Make sure you\n",
    "# understand the code before running it! If necessary, refer to the text in\n",
    "# Chapter 2.\n",
    "#\n",
    "###############################################################################\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, split, explode, lower, regexp_extract\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "book = spark.read.text(\"../data/gutenberg_books/1342-0.txt\")\n",
    "\n",
    "lines = book.select(split(book.value, \" \").alias(\"line\"))\n",
    "\n",
    "words = lines.select(explode(col(\"line\")).alias(\"word\"))\n",
    "\n",
    "words_lower = words.select(lower(col(\"word\")).alias(\"word_lower\"))\n",
    "\n",
    "words_clean = words_lower.select(\n",
    "    regexp_extract(col(\"word_lower\"), \"[a-z]*\", 0).alias(\"word\")\n",
    ")\n",
    "\n",
    "words_nonull = words_clean.where(col(\"word\") != \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "156f1724-4f7d-4b69-9e07-03805c2f68b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#book = spark.read.text(\"../data/gutenberg_books/1342-0.txt\")\n",
    "#book\n",
    "# DataFrame[value: string]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97073e56-08fc-434b-82bc-48cb7376ae3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|     word|\n",
      "+---------+\n",
      "|    start|\n",
      "|       of|\n",
      "|      the|\n",
      "|  project|\n",
      "|gutenberg|\n",
      "|    ebook|\n",
      "|   george|\n",
      "|    allen|\n",
      "|publisher|\n",
      "|  charing|\n",
      "+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "words_nonull.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cfda7c4d-c241-4b7b-9d97-9587126dbf7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The folder at results_single_partition.csv has been successfully deleted!\n",
      "+----+-----+\n",
      "|word|count|\n",
      "+----+-----+\n",
      "| the| 4617|\n",
      "|  to| 4292|\n",
      "|  of| 3831|\n",
      "| and| 3615|\n",
      "| her| 2254|\n",
      "|   a| 2007|\n",
      "|  in| 1964|\n",
      "| was| 1867|\n",
      "|   i| 1778|\n",
      "| she| 1703|\n",
      "+----+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "File deleted successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['word', 'count']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "# Import pathlib\n",
    "from pathlib import Path\n",
    "\n",
    "# Specify the file path\n",
    "folder_path = Path(\"./results_single_partition.csv\")\n",
    "\n",
    "# Check if the file exists before deleting it\n",
    "#if folder_path.exists():\n",
    "    # Delete the file\n",
    "#    folder_path.unlink()\n",
    "#    print(\"File deleted successfully.\")\n",
    "#else:\n",
    "#    print(\"File not found.\")\n",
    "    \n",
    "# Removing a non-empty folder with shutil.rmtree()\n",
    "import shutil\n",
    "\n",
    "# Delete folder using shutil\n",
    "try:\n",
    "    shutil.rmtree(folder_path)\n",
    "    print(f\"The folder at {folder_path} has been successfully deleted!\")\n",
    "except OSError as e:\n",
    "    print(f\"Error: {folder_path} - {e.strerror}.\")\n",
    "\n",
    "spark = SparkSession.builder.appName(\n",
    "    \"Counting word occurences from a book.\"\n",
    ").getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "#pyspark.sql.functions.explode\n",
    "\"\"\"\n",
    "    pyspark.sql.functions.explode(col: ColumnOrName) → pyspark.sql.column.Column[source]\n",
    "\n",
    "    Returns a new row for each element in the given array or map. \n",
    "    Uses the default column name col for elements in the array and key and value for elements in the map unless specified otherwise.\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "# If you need to read multiple text files, replace `1342-0` by `*`.\n",
    "results = (\n",
    "    spark.read.text(\"../data/gutenberg_books/1342-0.txt\")\n",
    "    .select(F.split(F.col(\"value\"), \" \").alias(\"line\"))\n",
    "    .select(F.explode(F.col(\"line\")).alias(\"word\"))\n",
    "    .select(F.lower(F.col(\"word\")).alias(\"word\"))\n",
    "    .select(F.regexp_extract(F.col(\"word\"), \"[a-z']*\", 0).alias(\"word\"))\n",
    "    .where(F.col(\"word\") != \"\")\n",
    "    .groupby(F.col(\"word\"))\n",
    "    .count()\n",
    ")\n",
    "\n",
    "results.orderBy(\"count\", ascending=False).show(10)\n",
    "results.coalesce(1).write.csv(\"./results_single_partition.csv\")\n",
    "\n",
    "\"\"\"\n",
    "When using toPandas(), remember that you lose the advantages of working with multiple machines, as the data will accumulate on the driver. \n",
    "Reserve this operation for an aggregated or manageable data set. While this is a crude formula, I usually take the number of rows times the number of columns; \n",
    "if this number is over 100,000 (for a 16 GB driver), I try to reduce it further. This simple trick helps me get a sense of the size of the data I am dealing with, \n",
    "as well as what’s possible given my driver size.\n",
    "\n",
    "You do not want to move your data between a pandas and a PySpark data frame all the time. \n",
    "Reserve toPandas() for either discrete operations or for moving your data into a pandas data frame once and for all. \n",
    "Moving back and forth will yield a ton of unnecessary work in distributing and collecting the data for nothing.\n",
    "\"\"\"\n",
    "\n",
    "if (os.path.exists(\"./results_single_partition_sg.csv\")):\n",
    "    os.remove(\"./results_single_partition_sg.csv\")\n",
    "    print(\"File deleted successfully.\")\n",
    "else:\n",
    "    results.coalesce(1).toPandas().to_csv(\"./results_single_partition_sg.csv\")\n",
    "\n",
    "results.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbcd1765-f563-4c91-bb84-2cbb5fbb183c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- split(value,  , -1): array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      "\n",
      "+--------------------+\n",
      "| split(value,  , -1)|\n",
      "+--------------------+\n",
      "|[***, START, OF, ...|\n",
      "|                  []|\n",
      "|                  []|\n",
      "|                  []|\n",
      "|                  []|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, split\n",
    "lines = book.select(split(col(\"value\"), \" \"))\n",
    "lines\n",
    "\n",
    "# DataFrame[split(value,  , -1): array<string>]\n",
    "lines.printSchema()\n",
    "# root\n",
    "#  |-- split(value,  , -1): array (nullable = true)\n",
    "#  |    |-- element: string (containsNull = true)\n",
    "lines.show(5)\n",
    "# +--------------------+\n",
    "# | split(value,  , -1)|\n",
    "# +--------------------+\n",
    "# |[The, Project, Gu...|\n",
    "# |                  []|\n",
    "# |[This, eBook, is,...|\n",
    "# |[almost, no, rest...|\n",
    "# |[re-use, it, unde...|\n",
    "# +--------------------+\n",
    "# only showing top 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fcee85-286f-475e-b13a-7aa5ae2f3ab9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
